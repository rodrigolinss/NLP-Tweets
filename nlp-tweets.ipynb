{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e06d8ed",
   "metadata": {
    "papermill": {
     "duration": 0.004599,
     "end_time": "2025-04-29T13:05:52.884242",
     "exception": false,
     "start_time": "2025-04-29T13:05:52.879643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Competição Kaggle: Natural Language Processing with Disaster Tweets\n",
    "\n",
    "\n",
    "## Abordagem 1: TF-IDF + Regressão Logística\n",
    "\n",
    "Este notebook implementa a primeira abordagem utilizada na competição, que consiste em:\n",
    "\n",
    "1. **Pré-processamento de Texto:** Limpeza básica dos tweets.\n",
    "2. **Vetorização TF-IDF:** Conversão do texto em uma matriz numérica usando Term Frequency-Inverse Document Frequency.\n",
    "3. **Modelo de Regressão Logística:** Treinamento de um classificador simples e eficiente.\n",
    "\n",
    "Esta abordagem obteve o melhor score (0.79313) entre todas as tentativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d89c67",
   "metadata": {
    "papermill": {
     "duration": 0.003461,
     "end_time": "2025-04-29T13:05:52.891663",
     "exception": false,
     "start_time": "2025-04-29T13:05:52.888202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Instalação das Bibliotecas Necessárias\n",
    "\n",
    "Execute esta célula para instalar as bibliotecas necessárias. Se você já as tem instaladas, pode pular esta etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec6c1a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:05:52.900335Z",
     "iopub.status.busy": "2025-04-29T13:05:52.899995Z",
     "iopub.status.idle": "2025-04-29T13:05:57.894210Z",
     "shell.execute_reply": "2025-04-29T13:05:57.892849Z"
    },
    "papermill": {
     "duration": 5.000736,
     "end_time": "2025-04-29T13:05:57.896181",
     "exception": false,
     "start_time": "2025-04-29T13:05:52.895445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn nltk numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b13fb5",
   "metadata": {
    "papermill": {
     "duration": 0.004161,
     "end_time": "2025-04-29T13:05:57.904710",
     "exception": false,
     "start_time": "2025-04-29T13:05:57.900549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Importação das Bibliotecas e Configuração Inicial\n",
    "\n",
    "Importamos todas as bibliotecas necessárias e configuramos o ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d181fcf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:05:57.914765Z",
     "iopub.status.busy": "2025-04-29T13:05:57.914057Z",
     "iopub.status.idle": "2025-04-29T13:06:01.895851Z",
     "shell.execute_reply": "2025-04-29T13:06:01.895081Z"
    },
    "papermill": {
     "duration": 3.98878,
     "end_time": "2025-04-29T13:06:01.897690",
     "exception": false,
     "start_time": "2025-04-29T13:05:57.908910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Baixar recursos do NLTK (stopwords) se ainda não tiver baixado\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e552b",
   "metadata": {
    "papermill": {
     "duration": 0.003917,
     "end_time": "2025-04-29T13:06:01.905958",
     "exception": false,
     "start_time": "2025-04-29T13:06:01.902041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Carregamento dos Dados\n",
    "\n",
    "Carregamos os arquivos `train.csv` e `test.csv` que contêm os tweets para classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97dfce25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:01.915621Z",
     "iopub.status.busy": "2025-04-29T13:06:01.914872Z",
     "iopub.status.idle": "2025-04-29T13:06:02.028377Z",
     "shell.execute_reply": "2025-04-29T13:06:02.027278Z"
    },
    "papermill": {
     "duration": 0.120182,
     "end_time": "2025-04-29T13:06:02.030181",
     "exception": false,
     "start_time": "2025-04-29T13:06:01.909999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando os dados...\n",
      "Tamanho do conjunto de treino: 7613 exemplos\n",
      "Tamanho do conjunto de teste: 3263 exemplos\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar os dados\n",
    "print(\"Carregando os dados...\")\n",
    "train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\n",
    "\n",
    "# Garantir que não há valores nulos na coluna 'text'\n",
    "train_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n",
    "test_df[\"text\"] = test_df[\"text\"].fillna(\"\")\n",
    "\n",
    "# Mostrar informações básicas\n",
    "print(f\"Tamanho do conjunto de treino: {train_df.shape[0]} exemplos\")\n",
    "print(f\"Tamanho do conjunto de teste: {test_df.shape[0]} exemplos\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46679b0",
   "metadata": {
    "papermill": {
     "duration": 0.004342,
     "end_time": "2025-04-29T13:06:02.039167",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.034825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. Pré-processamento do Texto\n",
    "\n",
    "Definimos e aplicamos uma função para limpar os textos dos tweets, removendo URLs, tags HTML, números, pontuação e stopwords (palavras comuns que geralmente não agregam valor semântico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e4c379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.049135Z",
     "iopub.status.busy": "2025-04-29T13:06:02.048778Z",
     "iopub.status.idle": "2025-04-29T13:06:02.289353Z",
     "shell.execute_reply": "2025-04-29T13:06:02.288378Z"
    },
    "papermill": {
     "duration": 0.247405,
     "end_time": "2025-04-29T13:06:02.290900",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.043495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pré-processando os textos...\n",
      "Exemplo original: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "Exemplo processado: deeds reason earthquake may allah forgive us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...   \n",
       "1             Forest fire near La Ronge Sask. Canada   \n",
       "2  All residents asked to 'shelter in place' are ...   \n",
       "3  13,000 people receive #wildfires evacuation or...   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                      processed_text  \n",
       "0       deeds reason earthquake may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  residents asked shelter place notified officer...  \n",
       "3  people receive wildfires evacuation orders cal...  \n",
       "4  got sent photo ruby alaska smoke wildfires pou...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Realiza a limpeza e pré-processamento de um texto.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Converter para minúsculas\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remover URLs\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Remover HTML tags\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "        # Remover números\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Remover pontuação\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # Remover stopwords\n",
    "        text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "        \n",
    "        # Remover espaços extras\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Aplicar pré-processamento\n",
    "print(\"Pré-processando os textos...\")\n",
    "train_df['processed_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['processed_text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar exemplos de texto processado\n",
    "print(\"Exemplo original:\", train_df['text'].iloc[0])\n",
    "print(\"Exemplo processado:\", train_df['processed_text'].iloc[0])\n",
    "\n",
    "train_df[['text', 'processed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431fd75",
   "metadata": {
    "papermill": {
     "duration": 0.004402,
     "end_time": "2025-04-29T13:06:02.300217",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.295815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Divisão dos Dados e Vetorização TF-IDF\n",
    "\n",
    "Dividimos os dados de treino em conjuntos de treino e validação (80% para treino, 20% para validação). Em seguida, usamos `TfidfVectorizer` para converter os textos processados em vetores numéricos.\n",
    "\n",
    "O TF-IDF (Term Frequency-Inverse Document Frequency) é uma técnica que atribui pesos às palavras baseando-se em sua frequência no documento e na raridade entre todos os documentos. Palavras comuns em um documento mas raras no corpus recebem pesos maiores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7144cf1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.310752Z",
     "iopub.status.busy": "2025-04-29T13:06:02.310436Z",
     "iopub.status.idle": "2025-04-29T13:06:02.323821Z",
     "shell.execute_reply": "2025-04-29T13:06:02.322769Z"
    },
    "papermill": {
     "duration": 0.020662,
     "end_time": "2025-04-29T13:06:02.325410",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.304748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do conjunto de treino: 6090\n",
      "Tamanho do conjunto de validação: 1523\n"
     ]
    }
   ],
   "source": [
    "# Separar features (texto processado) e target\n",
    "X = train_df['processed_text']\n",
    "y = train_df['target']\n",
    "\n",
    "# Dividir em treino e validação (80% treino, 20% validação)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Tamanho do conjunto de treino: {len(X_train)}\")\n",
    "print(f\"Tamanho do conjunto de validação: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29ee3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.336956Z",
     "iopub.status.busy": "2025-04-29T13:06:02.336048Z",
     "iopub.status.idle": "2025-04-29T13:06:02.466199Z",
     "shell.execute_reply": "2025-04-29T13:06:02.465092Z"
    },
    "papermill": {
     "duration": 0.137446,
     "end_time": "2025-04-29T13:06:02.467773",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.330327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando vetorização TF-IDF...\n",
      "Dimensões da matriz TF-IDF de treino: (6090, 5000)\n",
      "Dimensões da matriz TF-IDF de validação: (1523, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Vetorização TF-IDF\n",
    "print(\"Aplicando vetorização TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limitar o número de features para evitar overfitting\n",
    "    ngram_range=(1, 1)   # Apenas unigramas nesta abordagem inicial\n",
    ")\n",
    "\n",
    "# Ajustar o vetorizador aos dados de treino e transformar treino e validação\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "print(f\"Dimensões da matriz TF-IDF de treino: {X_train_tfidf.shape}\")\n",
    "print(f\"Dimensões da matriz TF-IDF de validação: {X_val_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30daf46f",
   "metadata": {
    "papermill": {
     "duration": 0.004751,
     "end_time": "2025-04-29T13:06:02.477507",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.472756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. Treinamento do Modelo de Regressão Logística\n",
    "\n",
    "Treinamos um modelo de Regressão Logística usando os vetores TF-IDF. A Regressão Logística é um algoritmo simples e eficiente para classificação binária, que funciona bem com dados de texto vetorizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff67fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.488100Z",
     "iopub.status.busy": "2025-04-29T13:06:02.487767Z",
     "iopub.status.idle": "2025-04-29T13:06:02.636608Z",
     "shell.execute_reply": "2025-04-29T13:06:02.635228Z"
    },
    "papermill": {
     "duration": 0.156223,
     "end_time": "2025-04-29T13:06:02.638389",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.482166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o modelo de Regressão Logística...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treinar o modelo de Regressão Logística\n",
    "print(\"Treinando o modelo de Regressão Logística...\")\n",
    "model = LogisticRegression(random_state=42, max_iter=1000) # Aumentar max_iter para garantir convergência\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8ea7e0",
   "metadata": {
    "papermill": {
     "duration": 0.007657,
     "end_time": "2025-04-29T13:06:02.654210",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.646553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. Avaliação do Modelo\n",
    "\n",
    "Avaliamos o desempenho do modelo treinado no conjunto de validação, calculando métricas como acurácia, F1-score e gerando um relatório de classificação completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0d18f2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.672099Z",
     "iopub.status.busy": "2025-04-29T13:06:02.671699Z",
     "iopub.status.idle": "2025-04-29T13:06:02.699843Z",
     "shell.execute_reply": "2025-04-29T13:06:02.699084Z"
    },
    "papermill": {
     "duration": 0.03915,
     "end_time": "2025-04-29T13:06:02.701735",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.662585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliando o modelo no conjunto de validação...\n",
      "Acurácia na validação: 0.8247\n",
      "F1-Score (classe 1) na validação: 0.7792\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.90      0.85       869\n",
      "           1       0.85      0.72      0.78       654\n",
      "\n",
      "    accuracy                           0.82      1523\n",
      "   macro avg       0.83      0.81      0.82      1523\n",
      "weighted avg       0.83      0.82      0.82      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fazer previsões no conjunto de validação\n",
    "print(\"Avaliando o modelo no conjunto de validação...\")\n",
    "y_pred_val = model.predict(X_val_tfidf)\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val) # F1-score para a classe positiva (1)\n",
    "report = classification_report(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Acurácia na validação: {accuracy:.4f}\")\n",
    "print(f\"F1-Score (classe 1) na validação: {f1:.4f}\")\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c870b",
   "metadata": {
    "papermill": {
     "duration": 0.007932,
     "end_time": "2025-04-29T13:06:02.717889",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.709957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8. Salvamento do Modelo e Vetorizador\n",
    "\n",
    "Salvamos o modelo treinado e o vetorizador TF-IDF para uso posterior na geração de previsões para o conjunto de teste ou para aplicações futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d7686f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.736166Z",
     "iopub.status.busy": "2025-04-29T13:06:02.735789Z",
     "iopub.status.idle": "2025-04-29T13:06:02.759650Z",
     "shell.execute_reply": "2025-04-29T13:06:02.758690Z"
    },
    "papermill": {
     "duration": 0.035179,
     "end_time": "2025-04-29T13:06:02.761214",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.726035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo e vetorizador salvos em: logistic_regression_model/logistic_regression_tfidf.pkl\n"
     ]
    }
   ],
   "source": [
    "# Criar diretório para salvar o modelo se não existir\n",
    "model_dir = 'logistic_regression_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Salvar modelo e vetorizador\n",
    "model_path = os.path.join(model_dir, 'logistic_regression_tfidf.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump((model, vectorizer), f)\n",
    "\n",
    "print(f\"Modelo e vetorizador salvos em: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d3eff",
   "metadata": {
    "papermill": {
     "duration": 0.00517,
     "end_time": "2025-04-29T13:06:02.771864",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.766694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 9. Geração de Previsões e Arquivo de Submissão\n",
    "\n",
    "Processamos o conjunto de teste e geramos o arquivo de submissão no formato exigido pelo Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655ea5b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.783318Z",
     "iopub.status.busy": "2025-04-29T13:06:02.782931Z",
     "iopub.status.idle": "2025-04-29T13:06:02.825950Z",
     "shell.execute_reply": "2025-04-29T13:06:02.825067Z"
    },
    "papermill": {
     "duration": 0.050459,
     "end_time": "2025-04-29T13:06:02.827483",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.777024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando os dados de teste...\n",
      "Dimensões da matriz TF-IDF de teste: (3263, 5000)\n",
      "Fazendo previsões no conjunto de teste...\n"
     ]
    }
   ],
   "source": [
    "# Usar o modelo e vetorizador já em memória\n",
    "loaded_model = model\n",
    "loaded_vectorizer = vectorizer\n",
    "\n",
    "# Preparar os dados de teste\n",
    "print(\"Preparando os dados de teste...\")\n",
    "X_test = test_df['processed_text']\n",
    "X_test_tfidf = loaded_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Dimensões da matriz TF-IDF de teste: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "print(\"Fazendo previsões no conjunto de teste...\")\n",
    "test_predictions = loaded_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed5d7878",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T13:06:02.840132Z",
     "iopub.status.busy": "2025-04-29T13:06:02.839429Z",
     "iopub.status.idle": "2025-04-29T13:06:02.858110Z",
     "shell.execute_reply": "2025-04-29T13:06:02.857203Z"
    },
    "papermill": {
     "duration": 0.02631,
     "end_time": "2025-04-29T13:06:02.859520",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.833210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criando arquivo de submissão...\n",
      "Arquivo de submissão salvo em: submission.csv\n",
      "Primeiras 10 linhas do arquivo de submissão:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar DataFrame de submissão\n",
    "print(\"Criando arquivo de submissão...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_predictions\n",
    "})\n",
    "\n",
    "# Garantir que os IDs estão na mesma ordem do arquivo de exemplo\n",
    "submission_df = submission_df.sort_values('id')\n",
    "\n",
    "# Salvar o arquivo de submissão\n",
    "submission_path = 'submission.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Arquivo de submissão salvo em: {submission_path}\")\n",
    "print(\"Primeiras 10 linhas do arquivo de submissão:\")\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b9fe4e",
   "metadata": {
    "papermill": {
     "duration": 0.005351,
     "end_time": "2025-04-29T13:06:02.870517",
     "exception": false,
     "start_time": "2025-04-29T13:06:02.865166",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 10. Conclusão\n",
    "\n",
    "Este notebook demonstrou a implementação de um modelo de Regressão Logística com vetorização TF-IDF para a classificação de tweets sobre desastres. Esta abordagem simples e eficiente obteve o melhor score (0.79313) entre todas as tentativas, superando inclusive modelos mais complexos como o DistilBERT (0.75543).\n",
    "\n",
    "Isso demonstra que, em muitos casos, modelos mais simples podem ser mais eficazes do que modelos complexos, especialmente quando o conjunto de dados é relativamente pequeno ou quando as características textuais são bem capturadas por técnicas como TF-IDF.\n",
    "\n",
    "O arquivo `submission_logistic_regression.csv` está pronto para ser enviado ao Kaggle."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15.448348,
   "end_time": "2025-04-29T13:06:03.494704",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-29T13:05:48.046356",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
